{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO\n",
    "- test with larger batches?\n",
    "- reduce data input (3 sec -> 1 sec)\n",
    "- run stylecheck + formatting to pretty up code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Link to dataset:\\\n",
    "https://github.com/mmalekzadeh/motion-sense/tree/master/data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this project has some unusual version dependencies.\n",
    "# you should run it within a fresh virtual environment\n",
    "# you really should use the requirements.txt, but this will work as well\n",
    "!pip3 install torch\n",
    "!pip3 install tqdm\n",
    "!pip3 install pandas\n",
    "!pip3 install scikit-learn\n",
    "!pip3 install tensorboard\n",
    "!pip3 install matplotlib\n",
    "!pip3 install seaborn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "import pathlib\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import numpy as np\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model settings and hyperparameters\n",
    "BATCH_SIZE = 8\n",
    "VALID_SPLIT = 0.20\n",
    "EPOCHS = 30\n",
    "LR = 0.00001\n",
    "NUMBER_OF_FRAMES_EXAMINED = 150 #50hz, 150=3sec\n",
    "MODEL_SAVE_DIR = \"/Users/isaac/Desktop/Creative/Coding/Transformers/\"\n",
    "DATASET_DIR = \"/Users/isaac/Desktop/Creative/Coding/Transformers/iPhone gyro\"\n",
    "dropout_p = 0.2\n",
    "device = torch.device(\"cpu\") # for mac, to use on GPU use \"cuda\" or \"cpu\" otherwise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch Transformer Demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a short test to import and verify the pytorch transformer (encoder + decoder) model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_model=512 # the number of expected features in the encoder/decoder inputs\n",
    "nhead=8 # the number of heads in the multiheadattention models\n",
    "num_encoder_layers=6 # the number of sub-encoder-layers in the encoder\n",
    "num_decoder_layers=6 # the number of sub-decoder-layers in the decoder\n",
    "dim_feedforward=2048 # the dimension of the feedforward network model\n",
    "dropout=0.1 # the dropout value\n",
    "activation=\"relu\" # the activation function of encoder/decoder intermediate layer\n",
    "custom_encoder=None # custom encoder\n",
    "custom_decoder=None # custom decoder\n",
    "layer_norm_eps=1e-05 # the eps value in layer normalization components\n",
    "batch_first=False # If True, then the input and output tensors are provided as (batch, seq, feature)\n",
    "norm_first=False # if True, encoder and decoder layers will perform LayerNorms before other attention and feedforward operations, otherwise after\n",
    "bias=True # If set to False, Linear and LayerNorm layers will not learn an additive bias\n",
    "device=None\n",
    "dtype=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer_model = torch.nn.Transformer(d_model=18, nhead=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test model inference\n",
    "src = torch.rand((30, 32, 18)) # (S,N,E)\n",
    "tgt = torch.rand((1, 32, 18)) # (T,N,E)\n",
    "out = transformer_model(src, tgt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 32, 18])"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape # (T, N, E)\n",
    "# S is the source sequence length\n",
    "# T is the target sequence length\n",
    "# N is the batch size\n",
    "# E is the feature number"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# iPhone Motion Detector Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderClassifier(torch.nn.Module):\n",
    "    def __init__(self, input_features, num_classifications, num_layers, num_heads, LR=0.00001, device=torch.device(\"cpu\")):\n",
    "        super().__init__()\n",
    "        # these are called sequentially in the forward() method\n",
    "        self.encoder_layer = torch.nn.TransformerEncoderLayer(\n",
    "            d_model=input_features,\n",
    "            nhead=num_heads,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.encoder = torch.nn.TransformerEncoder(\n",
    "            encoder_layer=self.encoder_layer,\n",
    "            num_layers=num_layers,\n",
    "        )\n",
    "        self.linear = torch.nn.Linear(input_features, num_classifications)\n",
    "        self.dropout = torch.nn.Dropout(dropout_p)\n",
    "        self.confusion_matrix = torch.zeros((num_classifications, num_classifications))\n",
    "        self.device = device\n",
    "        self.criterion = torch.nn.CrossEntropyLoss() # cross entropy for multiclass classification\n",
    "        self.optimizer = torch.optim.Adam(\n",
    "        self.parameters(),\n",
    "        lr=LR\n",
    "        ) # most used optimizer and learning rate with transformers from research I did\n",
    "        # self.writer = SummaryWriter() <--- causes pytorch error. instantiate every time :eyeroll:\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.dropout(x)\n",
    "        x = x.max(dim=1)[0] # condenses [N, S, E] to [N, E]\n",
    "        out = self.linear(x) # [N, E] -> [N, O]\n",
    "        return out\n",
    "\n",
    "\n",
    "    def postprocess(self, x, text_labels=False):\n",
    "        x = torch.nn.functional.softmax(x, dim=1) # apply softmax to model output (cross entropy loss does this automatically in training)\n",
    "        labels = x.max(dim=1)[1] # get classification for each batch of data\n",
    "        if text_labels: # convert [0, 0, 1, ...] to [\"dws\", \"dws\", \"ups\", ...]\n",
    "            action_dict = {\n",
    "            0 : \"dws\",\n",
    "            1 : \"ups\",\n",
    "            2 : \"wlk\",\n",
    "            3 : \"jog\",\n",
    "            4 : \"sit\",\n",
    "            5 : \"std\",\n",
    "        }\n",
    "            return [action_dict[label.item()] for label in labels]\n",
    "        return labels\n",
    "\n",
    "\n",
    "    # Training function.\n",
    "    def train_run(self, trainloader):\n",
    "        self.train()\n",
    "        print('Training')\n",
    "        train_running_loss = 0.0\n",
    "        train_running_correct = 0\n",
    "        counter = 0\n",
    "        for data in tqdm(trainloader, total=len(trainloader)):\n",
    "            counter += 1\n",
    "            inputs, labels = data['data'], data['label']\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.long().to(device)\n",
    "            self.optimizer.zero_grad()\n",
    "            # Forward pass\n",
    "            outputs = self(inputs)\n",
    "            labels = torch.squeeze(labels, -1) # [N, 1] -> [N] to match model outputs\n",
    "            # Calculate the loss.\n",
    "            loss = self.criterion(outputs, labels)\n",
    "            train_running_loss += loss.item()\n",
    "            SummaryWriter().add_scalar(\"train loss\", loss.item())\n",
    "            outputs = self.postprocess(outputs) # softmax + argmax output of model to get which action is predicted\n",
    "            running_correct = torch.sum(outputs == labels) # count correct predictions\n",
    "            train_running_correct += running_correct\n",
    "            # Backpropagation.\n",
    "            loss.backward()\n",
    "            # Update the optimizer parameters.\n",
    "            self.optimizer.step()\n",
    "\n",
    "        # Loss and accuracy for the complete epoch.\n",
    "        epoch_loss = train_running_loss / counter\n",
    "        epoch_acc = 100. * (train_running_correct / len(trainloader.dataset))\n",
    "        return epoch_loss, epoch_acc\n",
    "\n",
    "\n",
    "    # Validation function.\n",
    "    def validate(self, testloader):\n",
    "        self.eval()\n",
    "        print('Validation')\n",
    "        valid_running_loss = 0.0\n",
    "        valid_running_correct = 0\n",
    "        counter = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for data in tqdm(testloader, total=len(testloader)):\n",
    "                counter += 1\n",
    "                inputs, labels = data['data'], data['label']\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.long().to(device)\n",
    "                labels = torch.squeeze(labels, -1) # [N, 1] -> [N] to match model outputs\n",
    "                # Forward pass.\n",
    "                outputs = self(inputs)\n",
    "                # outputs = torch.squeeze(outputs, -1)\n",
    "                # Calculate the loss.\n",
    "                loss = self.criterion(outputs, labels)\n",
    "                valid_running_loss += loss.item()\n",
    "                outputs = self.postprocess(outputs) # softmax + argmax output of model to get which action is predicted\n",
    "                self.confusion_matrix += confusion_matrix(labels.flatten(), outputs.flatten(), labels=[0, 1, 2, 3, 4, 5])\n",
    "                running_correct = torch.sum(outputs == labels) # count correct predictions\n",
    "                valid_running_correct += running_correct\n",
    "\n",
    "        # Loss and accuracy for the complete epoch.\n",
    "        epoch_loss = valid_running_loss / counter\n",
    "        epoch_acc = 100. * (valid_running_correct / len(testloader.dataset))\n",
    "        return epoch_loss, epoch_acc\n",
    "\n",
    "\n",
    "    def process_confusion_matrix_class(self, class_num):\n",
    "        tp = self.confusion_matrix[class_num, class_num]\n",
    "        fp = self.confusion_matrix[class_num, :].sum() - tp\n",
    "        fn = self.confusion_matrix[:, class_num].sum() - tp\n",
    "        tn = self.confusion_matrix.sum() - tp - fp - fn\n",
    "        ap = tp / (tp + fp)\n",
    "        ar = tp / (tp + fn)\n",
    "        f1 = 2 / (1 / ar) + (1 / ap)\n",
    "        return tp, fp, tn, fn, ap, ar, f1\n",
    "\n",
    "    def process_confusion_matrix_avg(self):\n",
    "        classes = self.confusion_matrix.shape[0]\n",
    "        tp, fp, tn, fn, ap, ar, f1 = 0, 0, 0, 0, 0, 0, 0\n",
    "        scores = [tp, fp, tn, fn, ap, ar, f1]\n",
    "        for clas in range(classes):\n",
    "            scores = [old_score + new_score for old_score, new_score in zip(scores, self.process_confusion_matrix_class(clas))]\n",
    "        scores = [score/classes for score in scores]\n",
    "        return scores\n",
    "        # tp, fp, tn, fn, ap, ar, f1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new model\n",
    "model = EncoderClassifier(\n",
    "    input_features=18, # this is the combined number of features from iPhone gyroscope, movement, and acceleromoeter datasets\n",
    "    num_classifications=6, # walking downstairs, upstairs, walking, jogging, sitting, standing\n",
    "    num_layers=6,\n",
    "    num_heads=6,\n",
    ").float().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EncoderClassifier(\n",
       "  (encoder_layer): TransformerEncoderLayer(\n",
       "    (self_attn): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=18, out_features=18, bias=True)\n",
       "    )\n",
       "    (linear1): Linear(in_features=18, out_features=2048, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (linear2): Linear(in_features=2048, out_features=18, bias=True)\n",
       "    (norm1): LayerNorm((18,), eps=1e-05, elementwise_affine=True)\n",
       "    (norm2): LayerNorm((18,), eps=1e-05, elementwise_affine=True)\n",
       "    (dropout1): Dropout(p=0.1, inplace=False)\n",
       "    (dropout2): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): TransformerEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0-5): 6 x TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=18, out_features=18, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=18, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=2048, out_features=18, bias=True)\n",
       "        (norm1): LayerNorm((18,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((18,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (linear): Linear(in_features=18, out_features=6, bias=True)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       "  (criterion): CrossEntropyLoss()\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load model\n",
    "model = torch.load(MODEL_SAVE_DIR + 'model.pth')\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3, 4, 5, 5, 0, 4, 5, 1, 1, 1])\n",
      "['jog', 'sit', 'std', 'std', 'dws', 'sit', 'std', 'ups', 'ups', 'ups']\n",
      "tensor(6.1889, grad_fn=<DivBackward1>)\n"
     ]
    }
   ],
   "source": [
    "## print info if needed\n",
    "# print(model)\n",
    "# Total parameters and trainable parameters.\n",
    "# total_params = sum(p.numel() for p in model.parameters())\n",
    "# print(f\"{total_params:,} total parameters.\")\n",
    "# total_trainable_params = sum(\n",
    "#     p.numel() for p in model.parameters() if p.requires_grad)\n",
    "# print(f\"{total_trainable_params:,} training parameters.\\n\")\n",
    "\n",
    "# ensure model works\n",
    "input = torch.rand((10, 6, 18)).to(device)\n",
    "out = model(input)\n",
    "print(model.postprocess(out))\n",
    "print(model.postprocess(out, text_labels=True))\n",
    "assert out.shape == (10, 6)\n",
    "\n",
    "# ensure loss works\n",
    "labels = torch.rand((10, 6)).to(device)\n",
    "diffs = model.criterion(out, labels)\n",
    "print(diffs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROCESSING DATASET\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:27<00:00,  1.83s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NORMALIZING DATA AND SAVING TO /Users/isaac/Desktop/Creative/Coding/Transformers/iPhone gyro/whole_dataset_norm.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21/21 [00:00<00:00, 30.58it/s]\n"
     ]
    }
   ],
   "source": [
    "all_data = None\n",
    "action_dict = { # used in getting action info from dataset filenames\n",
    "    \"dws\": 0,\n",
    "    \"ups\": 1,\n",
    "    \"wlk\": 2,\n",
    "    \"jog\": 3,\n",
    "    \"sit\": 4,\n",
    "    \"std\": 5,\n",
    "}\n",
    "motion_path = Path(DATASET_DIR) / \"A_DeviceMotion_data\"\n",
    "accel_path = Path(DATASET_DIR) / \"B_Accelerometer_data\"\n",
    "gyro_path = Path(DATASET_DIR) / \"C_Gyroscope_data\"\n",
    "motion_iter = [file for file in motion_path.iterdir() if file.name != \".DS_Store\"] #.DS_Store appears on Mac and causes errors\n",
    "torch_dataset = [] # list of dicts (each dict is a batch) where each dict has \"data\" and \"label\" values\n",
    "print(\"PROCESSING DATASET\")\n",
    "for motion_action in tqdm(motion_iter):\n",
    "    accel_action = accel_path / motion_action.name\n",
    "    gyro_action = gyro_path / motion_action.name\n",
    "    action = motion_action.name.__str__().split(\"_\")[0] # getting action (standing, walking, etc.) from filename\n",
    "    trial = motion_action.name.__str__().split(\"_\")[1]\n",
    "    action_int = action_dict[action]\n",
    "    for motion_person in motion_action.glob('*.csv'):\n",
    "        person_idx = str(motion_person.stem).split(\"_\")[-1]\n",
    "        accel_person = accel_action / motion_person.name\n",
    "        gyro_person = gyro_action / motion_person.name\n",
    "        # dataset processing\n",
    "        motion_df = pd.read_csv(motion_person)\n",
    "        motion_len = len(motion_df)\n",
    "        accel_df = pd.read_csv(accel_person)\n",
    "        accel_df = accel_df.rename(columns={\"x\":\"accel_x\", \"y\":\"accel_y\", \"z\":\"accel_z\"})\n",
    "        accel_len = len(accel_df)\n",
    "        gyro_df = pd.read_csv(gyro_person)\n",
    "        gyro_df = gyro_df.rename(columns={\"x\":\"gyro_x\", \"y\":\"gyro_y\", \"z\":\"gyro_z\"})\n",
    "        gyro_len = len(gyro_df)\n",
    "        # motion, accelerometer, gyroscope dataset combination and processing\n",
    "        dataset_len = min(motion_len, accel_len, gyro_len)\n",
    "        person_df = pd.DataFrame(person_idx, index=np.arange(dataset_len), columns=[\"person_idx\"], dtype=int)\n",
    "        trial_df = pd.DataFrame(trial, index=np.arange(dataset_len), columns=[\"trial_number\"], dtype=int)\n",
    "        actions_df = pd.DataFrame(action_int, index=np.arange(dataset_len), columns=[\"action\"], dtype=int)\n",
    "        full_dataset = pd.concat([motion_df, accel_df, gyro_df, person_df, trial_df, actions_df], axis=1) # combine tables\n",
    "        full_dataset = full_dataset.drop([\"Unnamed: 0\"], axis=1) # remove index columns\n",
    "        full_dataset = full_dataset.iloc[:dataset_len]\n",
    "        if all_data is None:\n",
    "            all_data = full_dataset\n",
    "        else:\n",
    "            all_data = pd.concat([all_data, full_dataset])\n",
    "        # # splitting data into batches for data loader\n",
    "        # batches = len(full_dataset) // frames_per_batch\n",
    "        # for batch in range(1, batches+1):\n",
    "        #     batch_data = full_dataset.iloc[((batch - 1)*frames_per_batch):(batch*frames_per_batch)]\n",
    "        #     batch_data_tensor = torch.tensor(batch_data.values).to(torch.float32)\n",
    "        #     label_tensor = torch.tensor([action_int])\n",
    "        #     training_dict = {\"data\":batch_data_tensor, \"label\":label_tensor}\n",
    "        #     torch_dataset.append(training_dict)\n",
    "print(f\"NORMALIZING DATA\")\n",
    "dont_normalize = [\"person_idx\", \"trial_number\", \"action\"]\n",
    "for column_name in tqdm(all_data.columns):\n",
    "    if column_name in dont_normalize:\n",
    "        continue\n",
    "    all_data[column_name] -= all_data[column_name].min()\n",
    "    all_data[column_name] /= all_data[column_name].max()\n",
    "print(\"SAVING TO {DATASET_DIR + '/whole_dataset_norm.csv'}... will take some time\")\n",
    "all_data.to_csv(DATASET_DIR + \"/whole_dataset_norm.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset and Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOADING DATASET\n",
      "DATASET  LOADED\n",
      "PROCESSING DATASET\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:01<00:00,  7.80it/s]\n"
     ]
    }
   ],
   "source": [
    "# make pytorch dataset\n",
    "class iPhoneMotionDataset(Dataset):\n",
    "    def __init__(self, dataset_folder, frames_per_batch):\n",
    "        if type(dataset_folder) != pathlib.PosixPath: # conversion from str to Pathlib Path\n",
    "            dataset_folder = Path(dataset_folder)\n",
    "        print(\"LOADING DATASET\")\n",
    "        pd_dataset = pd.read_csv(dataset_folder / \"whole_dataset_norm.csv\")\n",
    "        self.torch_dataset = [] # list of dicts (each dict is a batch) where each dict has \"data\" and \"label\" values\n",
    "        trials = pd_dataset[\"trial_number\"].unique()\n",
    "        people = pd_dataset[\"person_idx\"].unique()\n",
    "        print(\"DATASET  LOADED\")\n",
    "        print(\"PROCESSING DATASET\")\n",
    "        for trial in tqdm(trials):\n",
    "            for person in people:\n",
    "                data = pd_dataset[(pd_dataset[\"trial_number\"] == trial) & (pd_dataset[\"person_idx\"] == person)]\n",
    "                labels = torch.tensor(data[\"action\"].values)\n",
    "                data = data.drop([\"trial_number\", \"person_idx\", \"action\", \"Unnamed: 0\"], axis=1)\n",
    "                data = torch.tensor(data.values)\n",
    "                batches = len(data) // frames_per_batch\n",
    "                for batch in range(1, batches+1):\n",
    "                    batch_data = data[((batch - 1)*frames_per_batch):(batch*frames_per_batch)].float()\n",
    "                    label_tensor = labels[(batch - 1)*frames_per_batch].float()\n",
    "                    training_dict = {\"data\":batch_data, \"label\":label_tensor}\n",
    "                    self.torch_dataset.append(training_dict)\n",
    "    def __len__(self):\n",
    "        return len(self.torch_dataset)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.torch_dataset[idx]\n",
    "\n",
    "iphone_motion_dataset = iPhoneMotionDataset(\n",
    "    dataset_folder=DATASET_DIR,\n",
    "    frames_per_batch=NUMBER_OF_FRAMES_EXAMINED\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data': tensor([[0.1632, 0.1570, 0.1401,  ..., 0.5651, 0.4814, 0.4742],\n",
       "         [0.1581, 0.1658, 0.1364,  ..., 0.5678, 0.4982, 0.4785],\n",
       "         [0.1572, 0.1732, 0.1344,  ..., 0.5802, 0.5229, 0.4826],\n",
       "         ...,\n",
       "         [0.5123, 0.1752, 0.8246,  ..., 0.5436, 0.5363, 0.5181],\n",
       "         [0.5126, 0.1549, 0.8213,  ..., 0.4930, 0.5399, 0.4773],\n",
       "         [0.5038, 0.1289, 0.8088,  ..., 0.4677, 0.5330, 0.4283]]),\n",
       " 'label': tensor(0.)}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iphone_motion_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting data into train and val datasets and dataloaders\n",
    "iphone_motion_train, iphone_motion_val = torch.utils.data.random_split(iphone_motion_dataset, [1 - VALID_SPLIT, VALID_SPLIT])\n",
    "iphone_motion_dataloader_train = DataLoader(iphone_motion_train, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n",
    "iphone_motion_dataloader_val = DataLoader(iphone_motion_val, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.zeros((3,8))\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 925/925 [02:49<00:00,  5.45it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1.4082970644976642, tensor(74.5199))"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.train_run(iphone_motion_dataloader_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 231/231 [00:05<00:00, 41.26it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.42002510172980173, tensor(90.6385))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.validate(iphone_motion_dataloader_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full Training Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "activate GUI with \\\n",
    "```tensorboard --logdir=runs```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO]: Epoch 1 of 30\n",
      "Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 925/925 [02:55<00:00,  5.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 231/231 [00:07<00:00, 29.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 6])\n",
      "Training loss: 0.4473708619620349, training acc: 87.12469482421875\n",
      "Validation loss: 0.4753989411509914, validation acc: 88.0952377319336\n",
      "Saving best model till now... LEAST LOSS 0.475\n",
      "--------------------------------------------------\n",
      "[INFO]: Epoch 2 of 30\n",
      "Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 925/925 [02:56<00:00,  5.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 231/231 [00:07<00:00, 32.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 6])\n",
      "Training loss: 0.42253752833282626, training acc: 87.96321105957031\n",
      "Validation loss: 0.4485046708093577, validation acc: 88.04113006591797\n",
      "Saving best model till now... LEAST LOSS 0.449\n",
      "--------------------------------------------------\n",
      "[INFO]: Epoch 3 of 30\n",
      "Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 925/925 [03:20<00:00,  4.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 231/231 [00:13<00:00, 17.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 6])\n",
      "Training loss: 0.4002319476854157, training acc: 88.7882080078125\n",
      "Validation loss: 0.4529113679221182, validation acc: 89.23160552978516\n",
      "--------------------------------------------------\n",
      "[INFO]: Epoch 4 of 30\n",
      "Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 50/925 [00:19<15:51,  1.09s/it]"
     ]
    }
   ],
   "source": [
    "# Lists to keep track of losses and accuracies.\n",
    "train_loss, valid_loss = [], []\n",
    "train_acc, valid_acc = [], []\n",
    "least_loss = float('inf')\n",
    "# Start the training.\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f\"[INFO]: Epoch {epoch+1} of {EPOCHS}\")\n",
    "    train_epoch_loss, train_epoch_acc = model.train_run(iphone_motion_dataloader_train)\n",
    "    valid_epoch_loss, valid_epoch_acc = model.validate(iphone_motion_dataloader_val)\n",
    "    train_loss.append(train_epoch_loss)\n",
    "    SummaryWriter().add_scalar(\"Train Epoch Loss\", train_epoch_loss, epoch)\n",
    "    SummaryWriter().add_scalar(\"Train Epoch Accuracy\", train_epoch_acc, epoch)\n",
    "    valid_loss.append(valid_epoch_loss)\n",
    "    SummaryWriter().add_scalar(\"Validation Epoch Loss\", valid_epoch_loss)\n",
    "    SummaryWriter().add_scalar(\"Validation Epoch Accuracy\", valid_epoch_acc)\n",
    "    print(model.confusion_matrix.shape)\n",
    "    fig = sn.heatmap(model.confusion_matrix, annot=True, ).get_figure()\n",
    "    fig.canvas.draw()\n",
    "    # Now we can save it to a numpy array.\n",
    "    image = np.frombuffer(fig.canvas.tostring_rgb(), dtype=np.uint8)\n",
    "    image = image.reshape(fig.canvas.get_width_height()[::-1] + (3,))\n",
    "    SummaryWriter().add_image(\"Confusion Matrix\", image.transpose((2, 0, 1)))\n",
    "    plt.clf()\n",
    "    tp, fp, tn, fn, ap, ar, f1 = model.process_confusion_matrix_avg()\n",
    "    SummaryWriter().add_scalar(\"True Positives\", tp, epoch)\n",
    "    SummaryWriter().add_scalar(\"False Positives\", fp, epoch)\n",
    "    SummaryWriter().add_scalar(\"False Negatives\", fn, epoch)\n",
    "    SummaryWriter().add_scalar(\"True Negatives\", tn, epoch)\n",
    "    SummaryWriter().add_scalar(\"Average Recall\", ar, epoch)\n",
    "    SummaryWriter().add_scalar(\"Average Precision\", ap, epoch)\n",
    "    SummaryWriter().add_scalar(\"F1 Score\", f1, epoch)\n",
    "\n",
    "    train_acc.append(train_epoch_acc)\n",
    "    valid_acc.append(valid_epoch_acc)\n",
    "    print(f\"Training loss: {train_epoch_loss}, training acc: {train_epoch_acc}\")\n",
    "    print(f\"Validation loss: {valid_epoch_loss}, validation acc: {valid_epoch_acc}\")\n",
    "    # Save model.\n",
    "    if valid_epoch_loss < least_loss:\n",
    "        least_loss = valid_epoch_loss\n",
    "        print(f\"Saving best model till now... LEAST LOSS {valid_epoch_loss:.3f}\")\n",
    "        torch.save(model, MODEL_SAVE_DIR + 'model.pth')\n",
    "    print('-'*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.3666, -0.0324, -1.9733, -0.1413, -0.6707],\n",
      "        [ 0.3253,  0.4910,  0.8005, -0.7587,  1.2424],\n",
      "        [-0.2742, -0.1118,  0.5459,  1.1000,  0.5414]], requires_grad=True)\n",
      "tensor([[0.0815, 0.4662, 0.3493, 0.0301, 0.0729],\n",
      "        [0.3190, 0.3179, 0.2653, 0.0166, 0.0812],\n",
      "        [0.2133, 0.5532, 0.0749, 0.0165, 0.1421]])\n"
     ]
    }
   ],
   "source": [
    "# # Example of target with class indices\n",
    "# loss = torch.nn.CrossEntropyLoss()\n",
    "# input = torch.randn(3, 5, requires_grad=True)\n",
    "# target = torch.empty(3, dtype=torch.long).random_(5)\n",
    "# print(input.shape)\n",
    "# print(target.shape)\n",
    "# print(input)\n",
    "# print(target)\n",
    "# output = loss(input, target)\n",
    "# output.backward()\n",
    "\n",
    "# Example of target with class probabilities\n",
    "input = torch.randn(3, 5, requires_grad=True)\n",
    "target = torch.randn(3, 5).softmax(dim=1)\n",
    "print(input)\n",
    "print(target)\n",
    "output = loss(input, target)\n",
    "output.backward()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
